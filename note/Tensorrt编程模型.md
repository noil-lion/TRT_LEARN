# Tensorrt关键概念
## API
python API 和C++ API
## 编程模型
1. 构造阶段（模型定义和优化）
2. 运行时阶段（推理）
## 基本流程
* 创建网络定义。
* 指定构建器的配置。
* 调用构建器来创建引擎。
***
* 反序列化构造好的引擎的.plan。
* 从引擎创建执行上下文。
* 填充输入缓冲区以进行推理。
* 调用enqueue()或者execute()在执行上下文上运行推理。

## 插件
用于支持一下trt本身不支持的操作的实现。在转换网络时，ONNX 解析器可以找到使用 TensorRT 的PluginRegistry创建和注册的插件。TensorRT 附带一个插件库，其中许多插件和一些附加插件的源代码可以在此处找到。[插件库](https://github.com/NVIDIA/TensorRT/tree/main/plugin)

## 精度控制
TensorRT默认选择的CUDA内核执行浮点计算的核函数是FP32精度实现，可通过配置BuilderFlag 选项来指示计算精度，降低计算精度值至FP16可以提高计算速度，对于对于输入动态范围约为 1 的正则化模型计算会产生明显加速。

## Float量化
Tensorrt可以将浮点数float值线性压缩并四舍五入到八位整数int,这样会显著提高算术吞吐量,但在量化float张量前,Tensorrt必须知道该数值的dynamic范围,动态范围信息可由构建器进行校准,也可以在框架中执行量化感知训练,最后将动态信息和模型一起导入Tensorrt.

## 张量和数据格式
Tensorrt会在优化网络的同时,在内部执行数据格式转换(.hwc,或其他),以使用尽可能快的cuda kernel.  
支持动态形状输入,具体实现通过指定构建器中一个模型实例的多个配置文件以生成优化的推理引擎, TensorRT 为每个配置文件创建一个优化的引擎，选择适用于 [最小、最大] 范围内的所有形状并且对于优化点最快的 CUDA 内核 - 通常每个配置文件都有不同的内核。

## trtexec
示例目录中包含一个名为trtexec的命令行包装工具。 trtexec是一种无需开发自己的应用程序即可快速使用 TensorRT 的工具。 trtexec工具有三个主要用途：  

* 在随机或用户提供的输入数据上对网络进行基准测试。
* 从模型生成序列化引擎。
* 从构建器生成序列化时序缓存。